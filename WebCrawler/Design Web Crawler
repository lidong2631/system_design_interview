Design web crawler

给定了api，
可以发送任务，任务内容是几个网址 eg a.com, b.cn，要求把上的所有图的地址找到，同时也包括该网址指向的其他网址上的图，以此递归
可以询问任务状态，告诉你比如说 1个任务完成了 一个任务还在做
可以查询任务的结果 {a.com: [http://xx.img1.png,http://yy.img2.png], b.c‍‌‌‍‌‍‍‌‍‌‍‍n: [http://cc.img3.jpg}

假设已经有一个工具， 给定一个网页，可以返回来网页里所有图片的地址和网页里的连接，这些连接也需要递归的处理
/ \
 |
如果是上面这个题可以按job scheduler来答

区别可能是对于每一个task url 还需要存domain 这样才能知道这个url爬下来的image属于哪个domain
job id 	|  domain  |  url  |  next trigger time  |  status  |  enqueued_time  |  enqueued_timeout  |  other status and their timeout  |  interval (for recursive job)  |  priority (optional prioritizer can compute that)







								DNS Resolver 					  Content Storage (web page storage. could use s3 or hdfs to store html)
		(task url queue: kafka)		| 			(optional)				|
seed URLs -> URL frontier -> HTML downloader -> Content Parser -> Content seen (hashing of the html code string and check if we have seen. Redis)
					|		(multi threaded crawler)					|
					|	(Robots.txt see which page can be crawled)	Link Extractor (can be extended for other module like Image Extractor, web monitor)
					|											  		|
					|											    URL Filter (optional)
					|											  		|
					| 												URL seen (hashtable / bloomfilter. Redis)
					|											  		|
				scheduler <---------------------------------------- URL Storage (nosql db refer to above jiuzhang design for table schema and psuedo codes)
				    /\   1. regularly check each domain and find one that can be crawled 
					| 	 2. check updated_at for each url and find one that can be crawled 
					| 	 3. send it to task queue
			web request (urls need to be crawl) send to scheduler

workflow:
1: Add seed URLs to the URL Frontier

2: HTML Downloader fetches a list of URLs from URL Frontier.

3: HTML Downloader gets IP addresses of URLs from DNS resolver and starts downloading.

4: Content Parser parses HTML pages and checks if pages are malformed.

5: After content is parsed and validated, it is passed to the “Content Seen?” component. 

Step 6: “Content Seen” component checks if a HTML page is already in the storage.
	• If it is in the storage, this means the same content in a different URL has already been processed. In this case, the HTML page is discarded.
	• If it is not in the storage, the system has not processed the same content before. The content is passed to Link Extractor.

7: Link extractor extracts links from HTML pages.

8: Extracted links are passed to the URL filter.

9: After links are filtered, they are passed to the “URL Seen?” component.

10: “URL Seen” component checks if a URL is already in the storage, if yes, it is processed before, and nothing needs to be done.

11: If a URL has not been processed before, it is added to the URL Frontier.


DB
DomainToUrlList 						DomainInfo 										UrlInfo
key = domain 							key = domain 									key = URL
value = [url1, url2 ...] 				value = <next_good_time, rate, ...> 			value = {doc_id, status, domain, created_At, updated_At }





web crawler跟job scheduler很像啊 ！！



---------- final solution:

ask question: 
are we crawling for html page only ?
what protocols we use, just http ?
how many pages we expect to crawl ?

爬取目标(面试要求): 10天之内抓取下 1B 网页 (1k webpages per sec) 需要 10T 的存储 (10k per webpage)

爬虫存HTML, 文本信息在不同的位置权重不同 标题和正文的权重不一样 且还需要保存 <a href=”/course/”> 这样的链接信息

seed URLs: 种子链接 (seed urls) 通常是一些新闻类网站 或者 Alexa 上的 Top 100 sites

Service: crawler service
使用什么算法进行爬取? BFS, most valuable info from web pages doesn't have much link depth

web crawler is producer and consumer 从队列中获取需要抓取的 URL 并抓取 把网页中解析出的新 URL 加入 到队列中

单进程(single process)爬虫是否可行? 最大瓶颈是什么? single process 会因为 network 的原因大部分时间处于 idle 状态 一般来说，平均 download 一篇 webpage 需要 2s 那么 single process 的性能只能做到 0.5 webpage / s

是否进程数越多越好? 既然一个 process 可以做到 0.5 webpage / s 是不是 2k 个 processes 就可以做到 1k webpages / s ? 
不行，过多的 context switch 会导致 CPU 利用率下降 更好的办法是，我们可以用 20 台机器，每台机器启动 100 个 processes，每个 process 单独执行一个爬虫程序

Storage
爬虫爬取到的网页，如何存储? DFS (Distributed File System) 若是直接在爬虫上存储会使得数据管理混乱 且破坏了爬虫 Stateless 的属性
BFS 中的队列如何存储? 直接在内存中存储会导断电时数据丢失 应该使用 Message Queue，如 Redis, Kafka, RabbitMQ
BFS 中的 HashSet 如何存储? 存储在数据库中 可以是效率比较高的 key-value 的数据库 除了是否被取过的信息，还可以同时存储其他的一些信息

Robots Exlcusion Protocol 协议 
有很多网站的 robots 协议中会限制爬虫的访问频率 Robots 协议不是一个强制协议，是一个软性约定

如何限制爬虫访问某个网站的频率? 让 Crawler 只做 Consumer，不负责产生新的抓取任务 新增一个 Scheduler (Producer) 负责调度和生产抓取任务 在 Database 中记录每个网站下一次可以友好抓取的时间
refer 第38页
Database 中存储的数据
DomainToUrlList 						DomainInfo 										UrlInfo
key = domain 							key = domain 									key = URL
value = [url1, url2 ...] 				value = <next_good_time, rate, ...> 			value = {doc_id, status, domain, created_At, updated_At }

def crawler_task(url):
	webpage = http_request.download(url) # download webpage
	distributed_file_system.save(url, webpage) # store webpage

	for next_url in extract_urls(webpage):
		if not database.url_existed(next_url):
			domain = fetch_domain(next_url) 	# get domain of the url
			database.append_url(domain=domain, url=next_url)  # put url into its domain

def scheduler(scheduler_id):
	while True:
		for domain in database.filter_domains(key=scheduler_id):
			if not good_time_to_crawl(domain):
				continue

			url = database.get_url_from(domain)
			message_queue.add_task('crawler_task', url)
			database.update_next_available_crawl_time(domain)

在不同的地区部署 Crawler 系统 每个地区只抓取所在地区被分配的 domain 下的网页 可以通过 domain 的 whois 信息来确定网站所属地区

如何处理网页的更新和失效? 增加对每个 URL 的信息记录 记录下这个 URL 下一次需要被重新抓取的时间 可以通过 Expenential Backoff 的方式计算
如何处理网页更新
• URL 抓取成功以后，默认 1 小时以后重新抓取
• 如果 1 小时以后抓取到的网页没有变化，则设为 2 小时以后再次抓取
• 2小时以后还是没有变化，则设为 4 小时以后重新抓取，以此类推
• 如果 1 小时以后抓取到的网页发生变化了，则设为 30 分钟以后再次抓取
• 30分钟以后又变化了，设为 15 分钟以后重新抓取。
• 设置抓取时间的上下边界，如至少 30 天要抓取一次，至多 5 分钟抓取一次
如何处理网页失效
• URL 抓取失效以后，默认 1 小时以后重新抓取
• 如果 1 小时以后依然抓不到，则设置为 2 小时
• 其他步骤类似网页更新的情况







Alex Xu's chapter 9 Design a Web Crawler

Candidate: What is the main purpose of the crawler? Is it used for search engine indexing, data mining, or something else?
Interviewer: Search engine indexing.

Candidate: How many web pages does the web crawler collect per month? 
Interviewer: 1 billion pages.

Candidate: What content types are included? HTML only or other content types such as PDFs and images as well?
Interviewer: HTML only.

Candidate: Shall we consider newly added or edited web pages? 
Interviewer: Yes, we should consider the newly added or edited web pages.

Candidate: Do we need to store HTML pages crawled from the web? 
Interviewer: Yes, up to 5 years

Candidate: How do we handle web pages with duplicate content? 
Interviewer: Pages with duplicate content should be ignored.

• Scalability: The web is very large. There are billions of web pages out there. Web crawling should be extremely efficient using parallelization.
• Robustness: The web is full of traps. Bad HTML, unresponsive servers, crashes, malicious links, etc. are all common. The crawler must handle all those edge cases.
• Politeness: The crawler should not make too many requests to a website within a short time interval.
• Extensibility: The system is flexible so that minimal changes are needed to support new content types. For example, if we want to crawl image files in the future, 
				we should not need to redesign the entire system.



high level design (I twisted a little with adding some parts from jiuzhang design)
FRESHNESS
• Recrawl based on web pages’ update history.
• Prioritize URLs and recrawl important pages first and more frequently.

Robustness
• Save crawl states and data: To guard against failures, crawl states and data are written to a storage system. A disrupted crawl can be restarted easily by loading saved states and data.
• short time out

Scale:
Distribute crawl servers geographically. When crawl servers are closer to website hosts, crawlers experience faster download time. Design locality applies to most of the system components: crawl servers, cache, queue, storage

								DNS Resolver 					  Content Storage (web page storage. could use s3 or hdfs to store html)
		(task url queue: kafka)		| 			(optional)				|
seed URLs -> URL frontier -> HTML downloader -> Content Parser -> Content seen (hashing of the html code string and check if we have seen. Redis)
					|		(multi threaded crawler)					|
					|	(Robots.txt see which page can be crawled)	Link Extractor (can be extended for other module like Image Extractor, web monitor)
					|											  		|
					|											    URL Filter (optional)
					|											  		|
					| 												URL seen (hashtable / bloomfilter. Redis)
					|											  		|
				scheduler <---------------------------------------- URL Storage (nosql db refer to above jiuzhang design for table schema and psuedo codes)
				    /\   1. regularly check each domain and find one that can be crawled 
					| 	 2. check updated_at for each url and find one that can be crawled 
					| 	 3. send it to task queue
			web request (urls need to be crawl) send to scheduler

Seed URLs: popular local websites or based on topics (shopping / sports / healthcare)

URL frontier: fifo queue

HTML downloader: urls provided by URL frontier
DNS Resolver: The HTML Downloader calls the DNS Resolver to get the corresponding IP address for the URL. For instance, URL www.wikipedia.org is converted to IP address 198.35.26.96 as of 3/5/2019

Content Parser: parsed and validated web page

Content Seen: compare hash version of two web pages

Content Storage

URL Extractor: parses and extracts links from HTML pages
URL filter
URL Seen: hashtable / bloomfilter
URL storage

workflow:
Step 1: Add seed URLs to the URL Frontier
Step 2: HTML Downloader fetches a list of URLs from URL Frontier.
Step 3: HTML Downloader gets IP addresses of URLs from DNS resolver and starts downloading.
Step 4: Content Parser parses HTML pages and checks if pages are malformed.
Step 5: After content is parsed and validated, it is passed to the “Content Seen?” component. Step 6: “Content Seen” component checks if a HTML page is already in the storage.
• If it is in the storage, this means the same content in a different URL has already been processed. In this case, the HTML page is discarded.
• If it is not in the storage, the system has not processed the same content before. The content is passed to Link Extractor.
Step 7: Link extractor extracts links from HTML pages.
Step 8: Extracted links are passed to the URL filter.
Step 9: After links are filtered, they are passed to the “URL Seen?” component.
Step 10: “URL Seen” component checks if a URL is already in the storage, if yes, it is processed before, and nothing needs to be done.
Step 11: If a URL has not been processed before, it is added to the URL Frontier.




deep dive
BFS over DFS since the depth of DFS can be very deep. Also useful information usually will 

BFS issue:
1. Most links from the same web page are linked back to the same host, Wikipedia servers will be flooded with requests
2. Standard BFS does not take the priority of a URL into consideration

URL frontier (refer to schedule servers from jiuzhang)

POLITENESS (??? I think jiuzhang design makes more sense. If we have a lot of hosts each of them has its own queue we might have millions queues is that applicable ?)
The general idea of enforcing politeness is to download one page at a time from the same host. A delay can be added between two download tasks. 
The politeness constraint is implemented by maintain a mapping from website hostnames to download (worker) threads.

• Queue router: It ensures that each queue (b1, b2, ... bn) only contains URLs from the same host.
• Mapping table: It maps each host to a queue.
	Host 			Queue
	wikipedia.com 	b1
	...
• FIFO queues b1, b2 to bn: Each queue contains URLs from the same host.
• Queue selector: Each worker thread is mapped to a FIFO queue, and it only downloads URLs from that queue. The queue selection logic is done by the Queue selector.
• Worker thread 1 to N. A worker thread downloads web pages one by one from the same host. A delay can be added between two download tasks.

PRIORITY
We prioritize URLs based on usefulness, which can be measured by PageRank [10], website traffic, update frequency, etc.

• Prioritizer: It takes URLs as input and computes the priorities.
• Queue f1 to fn: Each queue has an assigned priority. Queues with high priority are selected with higher probability.
• Queue selector: Randomly choose a queue with a bias towards queues with higher priority.

URL frontier design, and it contains two modules: 
• Front queues: manage prioritization
• Back queues: manage politeness

FRESHNESS
• Recrawl based on web pages’ update history.
• Prioritize URLs and recrawl important pages first and more frequently.

storage for URL frontier
The majority of URLs are stored on disk, so the storage space is not a problem. To reduce the cost of reading from the disk and writing to the disk, we maintain buffers in memory 
for enqueue/dequeue operations. Data in the buffer is periodically written to the disk.

HTML Downloader

Robots.txt - specifies what pages crawlers are allowed to download.
To avoid repeat downloads of robots.txt file, we cache the results of the file. The file is downloaded and saved to cache periodically

Performance optimization
1. Distributed crawl
To achieve high performance, crawl jobs are distributed into multiple servers, and each server runs multiple threads. The URL space is partitioned into smaller pieces; 
so, each downloader is responsible for a subset of the URLs

2. Cache DNS Resolver
DNS Resolver is a bottleneck for crawlers because DNS requests might take time due to the synchronous nature of many DNS interfaces
Once a request to DNS is carried out by a crawler thread, other threads are blocked until the first request is completed. Maintaining our DNS cache to 
avoid calling DNS frequently is an effective technique for speed optimization. Our DNS cache keeps the domain name to IP address mapping and is updated periodically by cron jobs.

3. Locality
Distribute crawl servers geographically. When crawl servers are closer to website hosts, crawlers experience faster download time. 

4. Short timeout
Some web servers respond slowly or may not respond at all. To avoid long wait time, a maximal wait time is specified. If a host does not respond within a predefined time, 
the crawler will stop the job and crawl some other pages.

Robustness
• Consistent hashing: This helps to distribute loads among downloaders. A new downloader server can be added or removed using consistent hashing. Refer to Chapter 5: Design consistent hashing for more details.
• Save crawl states and data: To guard against failures, crawl states and data are written to a storage system. A disrupted crawl can be restarted easily by loading saved states and data.

Extensibility
see pic

Detect and avoid problematic content
1. Redundant content
Hashes or checksums help to detect duplication

2. Spider traps
A spider trap is a web page that causes a crawler in an infinite loop 
www.spidertrapexample.com/foo/bar/foo/bar/foo/bar/...
Such spider traps can be avoided by setting a maximal length for URLs
a user can manually verify and identify a spider trap, and either exclude those websites from the crawler or apply some customized URL filters

3. Data noise
Some of the contents have little or no value, such as advertisements, code snippets, spam URLs, etc. Those contents are not useful for crawlers and should be excluded if possible.



additional points
• Server-side rendering: Numerous websites use scripts like JavaScript, AJAX, etc to generate links on the fly. If we download and parse web pages directly, we will not be able to retrieve dynamically generated links. To solve this problem, we perform server-side rendering (also called dynamic rendering) first before parsing a page.
• Filter out unwanted pages: With finite storage capacity and crawl resources, an anti-spam component is beneficial in filtering out low quality and spam pages.























